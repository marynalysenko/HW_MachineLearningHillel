{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10fcbfaa",
   "metadata": {},
   "source": [
    "# Classifying Phishing websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd092777",
   "metadata": {},
   "source": [
    "Classifying Phishing websites from Legitimate ones\n",
    "https://www.kaggle.com/datasets/aman9d/phishing-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8b1b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "# вывод всех резалтов, а не только последнего\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier \n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import xgboost as xgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "\n",
    "# Hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, accuracy_score, confusion_matrix, balanced_accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score as auc, roc_curve\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score as auc, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import shap\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e1376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"combined_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069630bd",
   "metadata": {},
   "source": [
    "| Feature | Description |\n",
    "| --- | --- |\n",
    "| Domain | The URL itself. |\n",
    "| Ranking | Page Ranking |\n",
    "| isIp | Is there an IP address in the weblink |\n",
    "| valid | This data is fetched from google's whois API that tells us more about the current status of the URL's registration. |\n",
    "| activeDuration | Also from whois API. Gives the duration of the time since the registration up until now. |\n",
    "| urlLen | It is simply the length of the URL |\n",
    "| is@ | If the link has a '@' character then it's value = 1 |\n",
    "| isredirect | If the link has double dashes, there is a chance that it is a redirect. 1-> multiple dashes present together. |\n",
    "| haveDash | If there are any dashes in the domain name. |\n",
    "| domainLen | The length of just the domain name. |\n",
    "| noOfSubdomain | The number of subdomains preset in the URL. |\n",
    "| Labels | 0 -> Legitimate website , 1 -> Phishing Link/ Spam Link |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf854ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_profiling.ProfileReport(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62be86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are adding here an column named 'ID'  \n",
    "df['ID'] = np.arange(len(df))\n",
    "\n",
    "#arranging that column  \n",
    "cols = list(df.columns)\n",
    "cols = [cols[-1]] + cols[:-1]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f3b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# create a new dataframe with counts of valid and invalid URLs\n",
    "valid_counts = df['valid'].value_counts().reset_index()\n",
    "valid_counts.columns = ['valid', 'count']\n",
    "\n",
    "# create a pie chart with colors for valid and invalid URLs\n",
    "fig = px.pie(valid_counts, values='count', names='valid', color='valid',\n",
    "             color_discrete_sequence=['#00CC96', '#FFA15A'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8499652e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"columns in Phishing Websites data\")\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "cat_data = [i for i in df.select_dtypes(include=np.object).columns]\n",
    "num_data = [i for i in df.select_dtypes(include=np.number).columns]\n",
    "print(\"categorical columns in Phishing Websites data\",cat_data)\n",
    "print(\"--------------------------------------------------------------------------------\")\n",
    "print(\"numerical columns in Phishing Websites data\",num_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ade43e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    " \n",
    "x=df.drop('label',axis=1)\n",
    "x= x.drop('domain', axis=1)\n",
    "x= x.drop('ID', axis=1)\n",
    "\n",
    "y=df['label']\n",
    "\n",
    "feature_names = x.columns\n",
    "\n",
    "# Нормализация признаков\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883bd6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cdd960",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model = CatBoostClassifier(iterations=50,\n",
    "                           learning_rate=0.2,\n",
    "                           od_type='Iter',\n",
    "                           verbose=25,\n",
    "                           depth=10,\n",
    "                           random_seed=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_predict = model.predict(X_test)\n",
    "print('CatBoost Accuracy Score is {:.5}'.format(accuracy_score(y_test, y_predict)))\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_predict)\n",
    "# Plot confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=matplotlib.colormaps[\"Blues\"])\n",
    "plt.show()\n",
    "#plt.savefig(\"confusion_matrix.png\")\n",
    "\n",
    "# Print classification report\n",
    "report = classification_report(y_test, y_predict)\n",
    "print(report)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31d0809",
   "metadata": {},
   "source": [
    "# Feature importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd238bf1",
   "metadata": {},
   "source": [
    "#### Сначала проанализирую фичи на корреляцию с таргетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde602cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all correlations and sort \n",
    "correlations_data = df.corr()['label'].sort_values()\n",
    "\n",
    "print(correlations_data.head(15), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc17bab6",
   "metadata": {},
   "source": [
    "### Feature Importance from Tree-Based Models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8f5572",
   "metadata": {},
   "source": [
    "Feature Importance from Tree-Based Models: a built-in feature importance metric that can be used to rank the importance of features. It works by calculating the total reduction of impurity that a feature causes when it is used to split a node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b19ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train to DataFrame\n",
    "x_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = model.get_feature_importance()\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_idx]\n",
    "sorted_feature_names = np.array(feature_names)[sorted_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511befa7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Sort features by importance\n",
    "sorted_idx = np.argsort(feature_importance)[::-1]\n",
    "sorted_feature_importance = feature_importance[sorted_idx]\n",
    "sorted_feature_names = np.array(feature_names)[sorted_idx]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(sorted_feature_names.shape[0]):\n",
    "    print(f\"{i+1}. {sorted_feature_names[i]} ({sorted_feature_importance[i]:.4f})\")\n",
    "   \n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_feature_importance)), sorted_feature_importance, align='center')\n",
    "plt.yticks(range(len(sorted_feature_importance)), sorted_feature_names)\n",
    "plt.gca().invert_yaxis()  # invert y-axis to show most important feature at the top\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (from Tree-Based Models:)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7871bcf2",
   "metadata": {},
   "source": [
    "### Shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c7fc7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create object that can calculate shap values\n",
    "explainer = shap.TreeExplainer(model)\n",
    "# Calculate Shap values\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap_importance = pd.DataFrame( \n",
    "                                  (feature_names, \n",
    "                                   shap_values))\n",
    "shap.summary_plot(shap_values , X_train, feature_names = feature_names, max_display=25, auto_size_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bdc83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Shap feature importance\n",
    "shap_feature_importance = np.abs(shap_importance).mean(axis=0)\n",
    "sorted_idx = np.argsort(shap_feature_importance)[::-1]\n",
    "sorted_feature_importance = shap_feature_importance[sorted_idx]\n",
    "sorted_feature_names = np.array(feature_names)[sorted_idx]\n",
    "\n",
    "# Print feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(sorted_feature_names.shape[0]):\n",
    "    print(f\"{i+1}. {sorted_feature_names[i]} ({sorted_feature_importance[i]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6d6c4e",
   "metadata": {},
   "source": [
    "### Permutation Importance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717728d2",
   "metadata": {},
   "source": [
    "Permutation importance is a model agnostic approach that can be used to calculate the feature importance of any trained model. It works by randomly permuting the values of a feature and measuring how much it affects the model's performance. If the model's performance drops significantly, then it is an important feature. If it doesn't drop much, then it is not that important. Here's how you can calculate permutation importance:\n",
    "makefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71906dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(model, X_train, y_train, n_jobs=-1)\n",
    "\n",
    "# Get sorted feature names and importance scores\n",
    "sorted_idx = perm_importance.importances_mean.argsort()[::-1]\n",
    "sorted_feature_names = feature_names[sorted_idx]\n",
    "sorted_importance = perm_importance.importances_mean[sorted_idx]\n",
    "\n",
    "# Print feature importance\n",
    "print(\"Permutation Importance:\")\n",
    "for i in range(len(sorted_feature_names)):\n",
    "    print(f\"{i+1}. {sorted_feature_names[i]} ({sorted_importance[i]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7c7c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_feature_importance)), sorted_feature_importance, align='center')\n",
    "plt.yticks(range(len(sorted_feature_importance)), sorted_feature_names)\n",
    "plt.gca().invert_yaxis()  # invert y-axis to show most important feature at the top\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance (permutation)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c4406c",
   "metadata": {},
   "source": [
    "### Дополнительный анализ качества полученной модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c63456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y_true, y_pred):\n",
    "    return 2 * auc(y_true, y_pred) - 1\n",
    "\n",
    "y_pred_train = model.predict_proba(X_train)[:, 1]\n",
    "y_pred_test = model.predict_proba(X_test)[:, 1] \n",
    "\n",
    "gini_train = gini(y_train, y_pred_train)\n",
    "gini_test = gini(y_test, y_pred_test) \n",
    "\n",
    "\n",
    "print(\"Тренировочный (train)  Джини:\", gini_train)\n",
    "print(\"Проверочный (test)   Джини:\", gini_test) \n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "x= np.linspace(0,1,1000)\n",
    "plt.plot(x, x, 'k-.')\n",
    "\n",
    "fp_train, tpr_train, thr_train = roc_curve(y_train, y_pred_train, sample_weight=None)\n",
    "plt.plot(fp_train, tpr_train, ms=2,label='Train Gini=%0.3f' % (gini_train))\n",
    "\n",
    "fp_test, tpr_test, thr_test = roc_curve(y_test, y_pred_test, sample_weight=None)\n",
    "plt.plot(fp_test, tpr_test, ms=2,label='Test Gini=%0.3f' % (gini_test))\n",
    " \n",
    "\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel(\"FP\")\n",
    "plt.ylabel(\"TP\")\n",
    "plt.legend(loc=4)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b810a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res, bins = pd.qcut(y_pred_train, q=10, duplicates='drop', retbins=True, labels=False)\n",
    "bins = np.concatenate(([-np.inf], bins[1:-1], [np.inf]))\n",
    "pd.cut(y_pred_train, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4324b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.DataFrame(dict(\n",
    "    y_true=y_test, \n",
    "    y_pred=y_pred_test\n",
    "))\n",
    "predictions['decile'] = pd.cut(predictions['y_pred'], bins=bins, labels=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b219a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "decile_table = predictions.groupby('decile').agg(dict(y_true=['count', 'sum', 'mean']))\n",
    "decile_table.columns = ['total', 'good', 'percent']\n",
    "decile_table['bad'] = decile_table['total'] - decile_table['good']\n",
    "decile_table.reset_index(inplace=True)\n",
    "decile_table['decile'] += 1\n",
    "\n",
    "# plot settings\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "axe1 = fig.add_subplot(111)\n",
    "p1 = axe1.bar(decile_table['decile'], decile_table['good'], 0.9, color='g')\n",
    "p2 = axe1.bar(decile_table['decile'], decile_table['bad'], 0.9, color='y', bottom=decile_table['good'])\n",
    "axe2 = fig.add_subplot(111, sharex=axe1, frameon=False)\n",
    "axe2.plot(decile_table['decile'], decile_table['percent'], marker='o')\n",
    "axe2.yaxis.tick_right()\n",
    "axe2.yaxis.set_label_position(\"right\")\n",
    "plt.xticks(decile_table['decile'])\n",
    "axe1.legend(('Good', 'Bad'), loc=0)\n",
    "plt.show()\n",
    "decile_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2138820",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby('decile').agg(dict(y_pred=['count', 'min', 'max']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
